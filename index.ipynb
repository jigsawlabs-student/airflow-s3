{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "technological-august",
   "metadata": {},
   "source": [
    "#  Uploading to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-parts",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-flush",
   "metadata": {},
   "source": [
    "In this lesson, we'll see how we can extract data from an RDS instance to load into an S3 instance.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-coaching",
   "metadata": {},
   "source": [
    "### Connecting to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-bathroom",
   "metadata": {},
   "source": [
    "Now connecting to S3, is a little trickier than our previous connections.  Here, we'll again have to go to `Admin > Connections`, and create a new connection.  From there, we can enter information that looks like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-upper",
   "metadata": {},
   "source": [
    "> <img src=\"./s3_connection.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-audit",
   "metadata": {},
   "source": [
    "So our `Conn Type` is `S3` and we leave the rest blank until we get to `Extra`.  There we add a dictionary with the keys of `aws_access_key_id` and `aws_secret_access_key`.  The values are the corresponding aws values from your acccount."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-organization",
   "metadata": {},
   "source": [
    "```python\n",
    "{\"aws_access_key_id\": \"...\", \"aws_secret_access_key\": \"...\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-democrat",
   "metadata": {},
   "source": [
    "### Creating a Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-substitute",
   "metadata": {},
   "source": [
    "Now that we've create the connection it's time to use the connection to upload some data to our s3 bucket.  We can do so with something like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-lawyer",
   "metadata": {},
   "source": [
    "```python\n",
    "from airflow.hooks.S3_hook import S3Hook\n",
    "\n",
    "def upload_to_s3():\n",
    "    s3_hook = S3Hook('s3_connection')\n",
    "    s3_hook.load_string(string_data='hello world', bucket_name='jk-jigsaw-foursquare', key='practice.csv')\n",
    "    \n",
    "rds_to_s3 = PythonOperator(task_id='rds_to_s3',\n",
    "                            dag = get_foursquare_info,\n",
    "                            python_callable = upload_to_s3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-intent",
   "metadata": {},
   "source": [
    "So here, we import the S3Hook, and then reference the connection via the `Conn Id` we specified in the Airflow webserver.  We use the `load_string` method to load the specified string to our bucket, and in a new file called `practice.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-catering",
   "metadata": {},
   "source": [
    "Ok, now let's see if it worked.  If we check the log of our data, we ccan see that `S3_hook` was called."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-diagnosis",
   "metadata": {},
   "source": [
    "> <img src=\"./uploading_s3.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-commodity",
   "metadata": {},
   "source": [
    "And if we look at our s3 buckets, we'll see a new `practice.csv` file loaded into our bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-bailey",
   "metadata": {},
   "source": [
    "> <img src=\"./practice_csv.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-employment",
   "metadata": {},
   "source": [
    "## From RDS to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-criticism",
   "metadata": {},
   "source": [
    "Ok, so now that we saw how to connect to S3 and add a new file to the bucket, let's do so again, but this time uploading a CSV file populated with data from our RDS instance. \n",
    "\n",
    "To do so, we'll create a CSV file in memory which we can then upload to S3.  Unfortunately this step is a little complicated.  It's ok, we'll push through."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-traffic",
   "metadata": {},
   "source": [
    "To start, we'll connect to our RDS instance, and select the records that we'd like to move to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_hook = PostgresHook('rds')\n",
    "zipcode_records = rds_hook.get_records(\"SELECT * FROM zipcodes;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-murder",
   "metadata": {},
   "source": [
    "> First, we create an in memory file with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beautiful-hundred",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.StringIO at 0x10df30700>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "mem_file = io.StringIO()\n",
    "mem_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-yield",
   "metadata": {},
   "source": [
    "Then we wrap the file in a csvwriter, which is a file that allows us to write ccsv values, specifying that the end of a line is where there's a line separation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "supposed-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_writer = csv.writer(mem_file, lineterminator=os.linesep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-height",
   "metadata": {},
   "source": [
    "Finally, we write our zipcode records, and convert the file to binary so that we can upload this to S3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-industry",
   "metadata": {},
   "source": [
    "```python\n",
    "csv_writer.writerows(zipcode_records)\n",
    "# encode into a byte stream\n",
    "mem_file_binary = io.BytesIO(mem_file.getvalue().encode())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-dominant",
   "metadata": {},
   "source": [
    "Finally, our s3_hook has a `load_file_obj` function that allows us to upload the file to an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_hook.load_file_obj(\n",
    "       file_obj=mem_file_binary,\n",
    "       bucket_name='jigsaw-sample-data',\n",
    "       key='sample_zipcodes.csv',\n",
    "       replace=True,\n",
    "   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-roots",
   "metadata": {},
   "source": [
    "Ok, now let's see the entire function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-europe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def load_to_s3():\n",
    "    rds_hook = PostgresHook('rds')\n",
    "    s3_hook = S3Hook('s3_connection')\n",
    "    zipcode_records = rds_hook.get_records(\"SELECT * FROM zipcodes;\")\n",
    "    # create an in memory file\n",
    "    mem_file = io.StringIO()\n",
    "    csv_writer = csv.writer(mem_file, lineterminator=os.linesep)\n",
    "    csv_writer.writerows(zipcode_records)\n",
    "    # encode into a byte stream\n",
    "    mem_file_binary = io.BytesIO(mem_file.getvalue().encode())\n",
    "    s3_hook.load_file_obj(\n",
    "       file_obj=mem_file_binary,\n",
    "       bucket_name='jigsaw-sample-data',\n",
    "       key='sample_zipcodes.csv',\n",
    "       replace=True,\n",
    "   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-plant",
   "metadata": {},
   "source": [
    "And after running this task in airflow, we'll find data from our RDS instance successfully uploaded to our S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-framework",
   "metadata": {},
   "source": [
    "> <img src=\"./uploaded_zipcodes.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-baltimore",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-helena",
   "metadata": {},
   "source": [
    "In this lesson, we saw how to select records from a database in RDS, convert those records into a CSV file, and then upload that CSV file to an S3 bucket.  To do so, we needed to create an S3 connection, which we again did through the airflow webserver, and then in the extra field, specified our aws keys with the following dictionary: `{\"aws_access_key_id\": \"...\", \"aws_secret_access_key\": \"...\"}`.\n",
    "\n",
    "After making a connection to S3, we wrote a task that selected records from an RDS, converted the records to an in memory CSV file, and uploaded the CSV file to S3.  To convert the records to an in memory CSV file, we used the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_file = io.StringIO()\n",
    "csv_writer = csv.writer(mem_file, lineterminator=os.linesep)\n",
    "\n",
    "csv_writer.writerows(zipcode_records)\n",
    "# encode into a byte stream\n",
    "mem_file_binary = io.BytesIO(mem_file.getvalue().encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-demand",
   "metadata": {},
   "source": [
    "This allowed us to call the `s3_hook.load_file_obj` to upload the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-western",
   "metadata": {},
   "source": [
    "### Resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-cooking",
   "metadata": {},
   "source": [
    "* [StringIO](geeksforgeeks.org/stringio-module-in-python/)\n",
    "* [Manning - Data Pipelines with Airflow ](https://livebook.manning.com/book/data-pipelines-with-apache-airflow/chapter-7/v-6/41)\n",
    "* [Airflow Lessons Learned Medium](https://medium.com/snaptravel/airflow-part-2-lessons-learned-793fa3c0841e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
